{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22f6cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoder is used to convert categorical data to numerical data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "# We may use Decision Tree or XGBoost for better results\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce8a833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So how does XGBoost work?\n",
    "# It builds an ensemble of weak learners (usually decision trees) in a sequential manner.\n",
    "# Each new tree is trained to correct the errors made by the previous trees.\n",
    "# The final prediction is made by combining the predictions of all the trees, typically through a weighted sum.\n",
    "# XGBoost is known for its speed and performance, making it a popular choice for many machine learning tasks.\n",
    "# Say that, there is an information_gain function that helps to decide the best feature to split the data at each node of the tree.\n",
    "# Information gain is a measure used in decision trees to determine which feature to split the data on at each node.\n",
    "# It quantifies the reduction in uncertainty or entropy about the target variable after splitting the data based on a particular feature.\n",
    "# The feature that provides the highest information gain is chosen for the split, as it helps to create more homogeneous subsets of data.\n",
    "# Information gain is calculated as the difference between the entropy of the original dataset and the weighted sum of the entropies of the subsets created by the split.\n",
    "# Entropy is a measure of uncertainty or impurity in the data, and it is calculated using the formula:\n",
    "# Entropy(S) = - Î£ (p(x) * log2(p(x)))\n",
    "# where p(x) is the proportion of instances belonging to class x in the dataset S.\n",
    "# The information gain for a feature A is then calculated as:\n",
    "# Information Gain(S, A) = Entropy(S) - Î£ (|Sv| / |S|) * Entropy(Sv)\n",
    "# where Sv is the subset of S for which feature A has value v, and |Sv| is the number of instances in Sv.\n",
    "# The feature with the highest information gain is selected for the split at that node in the decision\n",
    "# tree, as it helps to create more informative and effective splits that lead to better classification or regression performance.\n",
    "# Note: Information gain is commonly used in algorithms like ID3 and C4.5 for building decision trees.\n",
    "# However, XGBoost uses a more advanced approach called gradient boosting, which focuses on minimizing a specific loss function rather than relying solely on information gain.\n",
    "# In gradient boosting, the model is built in a sequential manner, where each new tree is trained to correct the errors made by the previous trees.\n",
    "# The objective is to minimize a loss function, such as mean squared error for regression tasks or log loss for classification tasks.\n",
    "# So, how did we do it? We used the information gain concept to guide the splitting of the data at each node of the trees.\n",
    "# This helps to create more informative splits that lead to better predictions.\n",
    "# However, the actual implementation of XGBoost involves more complex techniques, such as regularization, handling missing values, and parallel processing, to optimize the model's performance.\n",
    "# Which is something that we don't learn in basic ML courses.\n",
    "# So, in summary, while information gain is a useful concept for understanding how decision trees work, XGBoost goes beyond that by using gradient boosting to build a powerful ensemble of trees that can achieve high accuracy on various machine learning tasks.\n",
    "# Now, let's build this from scratch so that it can give you the whole concept of how it works without using the library.\n",
    "# Note: Implementing XGBoost from scratch is a complex task that requires a deep understanding of machine learning concepts and algorithms.\n",
    "# The following code provides a simplified version of XGBoost for educational purposes only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf58faf",
   "metadata": {},
   "source": [
    "### Let's build a Decision Tree Without library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c59229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that this is a basic implementation for a Decision Tree Regressor.\n",
    "# Let's begin.\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        self.feature = feature # Index of the feature to split on, example: 0 for the first feature and we will decide it based on information gain\n",
    "        # Like for an instance, it will split based on the feature value at index 0 whether it is greater than or less than the threshold value\n",
    "        self.threshold = threshold # Value to split the feature on, but we will decide it based on information gain\n",
    "        self.left = left # Left child node, in competitive programming, we usually use None for null values\n",
    "        self.right = right # Right child node, in competitive programming, we usually use None for null values\n",
    "        self.value = value # Value to return if the node is a leaf node\n",
    "    # So in conclusion, in a Decision tree, there will be features, and if a feature that got counted with information gain, and if it is greater than threshold, it will go to the left child node, otherwise, it will go to the right child node.\n",
    "    # If it is a leaf node, it will return the value.\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_feats = None):\n",
    "        self.min_samples_split = min_samples_split # Minimum number of samples required to split a node, how do we know this? We can decide it based on the dataset size\n",
    "        self.max_depth = max_depth # Maximum depth of the tree to prevent overfitting\n",
    "        self.n_feats = n_feats # Number of features to consider when looking for the best split # If None, then consider all features\n",
    "        self.root = None # Root node of the tree # Initially, it is None\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1]) # n_feats will take the number of features in X if it is None, otherwise, it will take the minimum of n_feats and the number of features in X\n",
    "        self.root = self._grow_tree(X, y) # Grow the tree starting from the root node\n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples >= self.min_samples_split and depth < self.max_depth:\n",
    "            feat_idxs = np.random.choice(n_features, self.n_feats, replace = False) # Randomly select n_feats features to consider for the best split\n",
    "            best_feat, best_thresh = self.best_criteria(X, y, feat_idxs) # Get the best feature and threshold to split on, we will learn about this function later\n",
    "            if best_feat is not None and best_thresh is not None: # Ensure both are not None\n",
    "                left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh) # Split the data based on the best feature and threshold\n",
    "                if len(left_idxs) > 0 and len(right_idxs) > 0: # Only split if both sides have samples\n",
    "                    left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1) # Recursively grow the left subtree\n",
    "                    right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1) # Recursively grow the right subtree\n",
    "                    return DecisionTreeNode(best_feat, best_thresh, left, right) # Return a decision node\n",
    "        leaf_value = self._most_common_value(y) # If we cannot split further, return the most common value in y\n",
    "        return DecisionTreeNode(value = leaf_value) # Return a leaf node\n",
    "    def best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1 # Initialize the best gain to -1\n",
    "        split_idx, split_thresh = None, None # Initialize the best feature index and threshold to None\n",
    "        for feat_idx in feat_idxs: # Iterate over the selected feature indices\n",
    "            X_column = X[:, feat_idx] # Get the column of the feature to split on\n",
    "            thresholds = np.unique(X_column) # Get the unique values of the feature to use as thresholds\n",
    "            for threshold in thresholds: # Iterate over the unique thresholds\n",
    "                gain = self._information_gain(y, X_column, threshold) # Calculate the information gain for the split\n",
    "                if gain > best_gain: # If the gain is better than the best gain found so far\n",
    "                    best_gain = gain # Update the best gain\n",
    "                    split_idx = feat_idx # Update the best feature index\n",
    "                    split_thresh = threshold # Update the best threshold\n",
    "        return split_idx, split_thresh # Return the best feature index and threshold\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # Parent entropy\n",
    "        parent_entropy = self._entropy(y) # Calculate the entropy of the parent node\n",
    "        # Generate split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh) # Split the data based on the threshold\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0: # If one of the splits is empty, return 0 gain\n",
    "            return 0\n",
    "        # Weighted average child entropy\n",
    "        n = len(y) # Total number of samples\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs) # Number of samples in the left and right splits\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs]) # Entropy of the left and right splits\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r # Weighted average of the child entropies\n",
    "        # Information gain is difference in entropy before vs. after split\n",
    "        ig = parent_entropy - child_entropy # Calculate the information gain\n",
    "        return ig # Return the information gain\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten() # Get the indices of the samples that go to the left split\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten() # Get the indices of the samples that go to the right split\n",
    "        return left_idxs, right_idxs # Return the indices of the left and right splits\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y) # Count the occurrences of each class in y\n",
    "        ps = hist / len(y) # Calculate the probabilities of each class\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0]) # Calculate and return the entropy\n",
    "    \n",
    "    # Entropy is a measure of uncertainty or impurity in the data, and it is calculated using the formula:\n",
    "    # Entropy(S) = - Î£ (p(x) * log2(p(x)))\n",
    "    # where p(x) is the proportion of instances belonging to class x in the dataset S\n",
    "    # The information gain for a feature A is then calculated as:\n",
    "    # Information Gain(S, A) = Entropy(S) - Î£ (|Sv| / |S|) * Entropy(Sv)\n",
    "    # where Sv is the subset of S for which feature A has value v, and |Sv| is the number of instances in Sv.\n",
    "    # The feature with the highest information gain is selected for the split at that node in the decision\n",
    "    # tree, as it helps to create more informative and effective splits that lead to better classification or regression performance.\n",
    "    # Note: Information gain is commonly used in algorithms like ID3 and C4.5 for building decision trees.\n",
    "\n",
    "\n",
    "    def _most_common_value(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None  # or you can choose a default value, e.g., 0\n",
    "        counter = np.bincount(y) # Count the occurrences of each class in y\n",
    "        most_common = np.argmax(counter) # Get the class with the highest count\n",
    "        return most_common # Return the most common class\n",
    "    # So, in conclusion, the _most_common_value function is used to determine the most frequently occurring class label in the target variable y. This is particularly useful when creating leaf nodes in a decision tree, where we want to assign the most common class label to the leaf node when further splitting is not possible or necessary.\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X]) # Traverse the tree for each sample in X and return the predictions\n",
    "    # The predict function takes a 2D array X as input, where each row represents a sample with multiple features. It iterates over each sample (row) in X and calls the _traverse_tree function to navigate through the decision tree starting from the root node. The _traverse_tree function returns the predicted value for each sample, and the predict function collects these predictions into a NumPy array and returns it.\n",
    "    # This allows us to make predictions for multiple samples at once by leveraging the structure of the decision tree.\n",
    "    # Note: The predict function assumes that the decision tree has already been trained and is ready for making predictions.\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None: # If the node is a leaf node\n",
    "            return node.value # Return the value of the leaf node\n",
    "        if x[node.feature] <= node.threshold: # If the feature value is less than or equal to the threshold\n",
    "            return self._traverse_tree(x, node.left) # Traverse the left subtree\n",
    "        return self._traverse_tree(x, node.right) # Traverse the right subtree\n",
    "# The _traverse_tree function is a recursive function that navigates through the decision tree to make a prediction for a single sample x. It starts at the given node and checks if the node is a leaf node (i.e., it has a value). If it is a leaf node, it returns the value of that node as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07727a2f",
   "metadata": {},
   "source": [
    "# ğ³ Decision Tree Regressor â Step-by-Step Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ğ§© 1. Introduction\n",
    "A **Decision Tree Regressor** predicts a continuous target value by recursively splitting data based on features that minimize impurity (e.g., variance, entropy, or Gini).  \n",
    "\n",
    "Each internal node represents a **decision rule** (feature and threshold),  \n",
    "and each leaf node represents a **predicted value**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğ§± 2. Node Structure (`DecisionTreeNode` Class)\n",
    "\n",
    "Each node in the tree contains:\n",
    "\n",
    "| Attribute | Description |\n",
    "|------------|-------------|\n",
    "| `feature` | Index of the feature used for the split |\n",
    "| `threshold` | The numerical value used to split the feature |\n",
    "| `left` | Pointer to the left child node (samples â¤ threshold) |\n",
    "| `right` | Pointer to the right child node (samples > threshold) |\n",
    "| `value` | The predicted value (for leaf nodes) |\n",
    "\n",
    "A node is a **leaf** if it has a `value` and no children.\n",
    "\n",
    "---\n",
    "\n",
    "## âï¸ 3. Decision Tree Regressor (`DecisionTreeRegressor` Class)\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|------------|-------------|\n",
    "| `min_samples_split` | Minimum samples required to split a node (prevents overfitting) |\n",
    "| `max_depth` | Maximum tree depth (controls complexity) |\n",
    "| `n_feats` | Number of features to consider at each split (adds randomness) |\n",
    "| `root` | Root node of the tree, built after training |\n",
    "\n",
    "---\n",
    "\n",
    "## ğ 4. Training the Model (`fit` Method)\n",
    "\n",
    "1. Determine the number of features.  \n",
    "2. Begin recursive tree construction using `_grow_tree()`.  \n",
    "3. Find the **best feature** and **threshold** at each node to maximize **information gain**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğ¿ 5. Growing the Tree (`_grow_tree` Method)\n",
    "\n",
    "1. Check stopping conditions:\n",
    "   - Fewer than `min_samples_split` samples, or  \n",
    "   - Current depth â¥ `max_depth`.\n",
    "\n",
    "2. Randomly select a subset of features (`feat_idxs`).  \n",
    "3. Find the **best split** using `best_criteria()`.  \n",
    "4. If a valid split is found:\n",
    "   - Split the data into left and right subsets.  \n",
    "   - Recursively grow the left and right subtrees.\n",
    "\n",
    "5. If no split improves information gain â create a **leaf node** using `_most_common_value(y)`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğ 6. Finding the Best Split (`best_criteria` Method)\n",
    "\n",
    "Loop through:\n",
    "- Each feature `feat_idx` in the selected subset.  \n",
    "- Each unique value in that feature (potential thresholds).\n",
    "\n",
    "For every `(feature, threshold)` pair:\n",
    "- Compute **information gain** using `_information_gain()`.  \n",
    "- Keep the pair that yields the **highest gain**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğ¡ 7. Information Gain (`_information_gain` Method)\n",
    "\n",
    "**Goal:** Measure how much a split reduces impurity.\n",
    "\n",
    "**Steps:**\n",
    "1. Compute parent node entropy:\n",
    "\n",
    "\\[\n",
    "Entropy(S) = - \\sum p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "2. Split data into left and right subsets.  \n",
    "3. Compute weighted average entropy of children.  \n",
    "4. Calculate Information Gain:\n",
    "\n",
    "\\[\n",
    "IG = Entropy(parent) - \\left( \\frac{n_L}{n} Entropy(left) + \\frac{n_R}{n} Entropy(right) \\right)\n",
    "\\]\n",
    "\n",
    "> ğ¸ In regression trees, this could be replaced by **variance reduction** instead of entropy.\n",
    "\n",
    "---\n",
    "\n",
    "## âï¸ 8. Splitting Data (`_split` Method)\n",
    "\n",
    "Divide the dataset into two groups based on a threshold:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfa1de",
   "metadata": {},
   "source": [
    "## ğ 9. Entropy Calculation (`_entropy` Method)\n",
    "\n",
    "The **entropy** of a dataset measures its impurity or uncertainty â how \"mixed\" the classes are.  \n",
    "\n",
    "In a binary classification problem:\n",
    "- If all samples belong to one class â entropy = 0 (pure)\n",
    "- If samples are evenly split â entropy = 1 (maximally uncertain)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( c \\) = number of unique classes  \n",
    "- \\( p_i \\) = probability of class \\( i \\) in the dataset  \n",
    "\n",
    "**Example:**\n",
    "If a node has 3 samples of class 0 and 1 sample of class 1:\n",
    "\n",
    "\\[\n",
    "p_0 = \\frac{3}{4}, \\quad p_1 = \\frac{1}{4}\n",
    "\\]\n",
    "\\[\n",
    "Entropy = - \\left( \\frac{3}{4}\\log_2\\frac{3}{4} + \\frac{1}{4}\\log_2\\frac{1}{4} \\right) = 0.811\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ğ§  10. Leaf Node Value (`_most_common_value` Method)\n",
    "\n",
    "When no further splitting improves the information gain,  \n",
    "the node becomes a **leaf node**, and its value is the **most common class** (for classification)  \n",
    "or the **average target value** (for regression).\n",
    "\n",
    "**Implementation Example:**\n",
    "\n",
    "```python\n",
    "def _most_common_value(self, y):\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    return values[np.argmax(counts)]\n",
    "```\n",
    "\n",
    "### ğ® 11. Prediction (predict Method)\n",
    "\n",
    "Once the tree is trained, we use it to predict outcomes by traversing from the root down to a leaf node for each sample.\n",
    "\n",
    "```python \n",
    "def predict(self, X):\n",
    "    return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "```\n",
    "\n",
    "Each sample is passed through the _traverse_tree() method until it reaches a leaf value.\n",
    "\n",
    "### ğ§­ 12. Tree Traversal (_traverse_tree Method)\n",
    "\n",
    "At each node: Check if itâs a leaf node â if yes, return its stored value.\n",
    "\n",
    "Otherwise, compare the feature with the threshold and go left or right.\n",
    "```python\n",
    "def _traverse_tree(self, x, node):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return self._traverse_tree(x, node.left)\n",
    "    return self._traverse_tree(x, node.right)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6fc9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's test our Decision Tree Regressor with the Bank Marketing dataset.\n",
    "path = kagglehub.dataset_download(\"henriqueyamahata/bank-marketing\")\n",
    "data = pd.read_csv(path + \"/bank-additional-full.csv\", sep=';')\n",
    "data.head()\n",
    "# print(path)\n",
    "\n",
    "X = data.drop('y', axis=1) #Or, you can use X = data[Categorical_features]\n",
    "y = data['y']\n",
    "# Note: Make sure to preprocess the data (handle categorical variables, missing values, etc.) before using it for training.\n",
    "# For simplicity, let's assume X and y are already preprocessed and ready for training.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#ValueError: could not convert string to float: 'blue-collar'\n",
    "# This error occurs because the StandardScaler is trying to convert string values (categorical data) to float, which is not possible.\n",
    "# To fix this, we need to preprocess the categorical features in the dataset before applying StandardScaler.\n",
    "# One common approach is to use one-hot encoding or label encoding to convert categorical variables into numerical format. \n",
    "# Say, you have One-Hot encoder. \n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder()\n",
    "# X_encoded = ohe.fit_transform(X)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "# Let's use Label Encoding for simplicity\n",
    "label_encoders = {}\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "    label_encoders[column] = le\n",
    "#But, what is with the complicated label encoding here? \n",
    "# We are storing the label encoders for each categorical column in a dictionary called label_encoders. This way, we can easily access the encoder for a specific column later if needed (for example, when making predictions on new data).\n",
    "# This is useful because we might want to convert the encoded values back to their original categorical format some point, or we might want to apply the same encoding to new data that we want to predict on.\n",
    "# Now, we can safely split the data and apply StandardScaler.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4112deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.67%\n"
     ]
    }
   ],
   "source": [
    "# Encode y_train and y_test to integers\n",
    "if 'y' not in label_encoders:\n",
    "\tlabel_encoders['y'] = LabelEncoder()\n",
    "\tlabel_encoders['y'].fit(y)\n",
    "\n",
    "y_train_enc = label_encoders['y'].transform(y_train)\n",
    "y_test_enc = label_encoders['y'].transform(y_test)\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=10)\n",
    "model.fit(X_train, y_train_enc)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_enc, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "#90% accuracy? Nice! But, can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.65%\n"
     ]
    }
   ],
   "source": [
    "# Now, let's use the library-based Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "model.fit(X_train, y_train_enc)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_enc, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a8e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vpthi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:35:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's use XGBoos for better results\n",
    "from xgboost import XGBClassifier  \n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss') # Suppress the warning about label encoding\n",
    "model.fit(X_train, y_train_enc)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_enc, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd574e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

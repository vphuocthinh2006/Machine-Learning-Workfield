{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ed7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoder is used to convert categorical data to numerical data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "# We may use Decision Tree or XGBoost for better results\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795c91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is tree ensemble methods?\n",
    "# Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "# Examples of ensemble methods include bagging, boosting, and stacking.\n",
    "# Decision trees are a type of supervised learning algorithm that can be used for both classification and regression tasks.\n",
    "# They work by recursively splitting the data into subsets based on the values of the input features, with the goal of creating homogeneous subsets that are easier to predict.\n",
    "# Tree ensemble methods combine multiple decision trees to improve the overall performance of the model.\n",
    "# Random Forest is an example of a tree ensemble method that uses bagging to create multiple decision trees and combines their predictions to make a final prediction.\n",
    "# Gradient Boosting is another tree ensemble method that builds decision trees sequentially, with each tree trying to correct the errors of the previous tree.\n",
    "# The final prediction is made by combining the predictions of all the trees in the ensemble.\n",
    "# Tree ensemble methods can often achieve better performance than individual decision trees, as they can reduce overfitting and improve generalization to new data.\n",
    "# However, they can also be more complex and computationally expensive to train and deploy.\n",
    "# Overall, tree ensemble methods are a powerful tool for improving the performance of decision tree models and are widely used in machine learning applications.\n",
    "# Examples of tree ensemble methods include Random Forest, Gradient Boosting, and XGBoost.\n",
    "# These methods can be used for both classification and regression tasks and are particularly effective for handling complex datasets with many features and interactions.\n",
    "# Tree ensemble methods are commonly used in various applications, such as fraud detection, customer churn prediction, and recommendation systems.\n",
    "# They are also popular in data science competitions, such as those hosted on Kaggle, where they often achieve top performance.\n",
    "# However, tree ensemble methods can also be more difficult to interpret than individual decision trees, as they involve multiple trees and complex interactions between features.\n",
    "# Therefore, it is important to carefully evaluate the performance and interpretability of tree ensemble models before deploying them in real-world applications.\n",
    "# Overall, tree ensemble methods are a powerful and widely used tool in machine learning that can help improve the accuracy and robustness of predictive models.\n",
    "# Anyways, let's implement Gradient Boosting Classifier from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifierScratch:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model with the mean of the target variable\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        #Find the mean of the true y values\n",
    "        # Initialize the residuals\n",
    "        residuals = y - self.initial_prediction\n",
    "        # Then y - mean(y) as to what the residuals are and what are residuals? \n",
    "        #Residuals are the difference between the true values and the predicted values.\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            # Predict the residuals\n",
    "            predictions = tree.predict(X)\n",
    "            # Update the residuals\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            # Store the tree\n",
    "            self.trees.append(tree)\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction)\n",
    "        # Add the predictions from each tree\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        # Convert to binary predictions\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "# The above class implements a simple version of Gradient Boosting Classifier from scratch using decision trees as base learners.\n",
    "# It includes methods for fitting the model to training data and making predictions on new data.\n",
    "# Note that this is a basic implementation and may not include all the optimizations and features of more advanced libraries like XGBoost or LightGBM.\n",
    "# Let's use this class to train a model on the loan prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "482ce3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"uciml/default-of-credit-card-clients-dataset\")\n",
    "df = pd.read_csv(path + '/UCI_Credit_Card.csv')\n",
    "# print(\"Path to dataset files:\", path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c00f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "# The dataset contains information about credit card clients and whether they defaulted on their payments.\n",
    "# And, this will predict if the client will get into a debt or not.\n",
    "# So, let's create a feature whereas we will predict if the client will default or not.\n",
    "# df['default_payment_next_month'] = df['default.payment.next.month']\n",
    "X = df.drop(columns=['ID', 'default.payment.next.month'], axis = 1)\n",
    "y = df['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f50afd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient Boosting Classifier from scratch: 0.821\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "model = GradientBoostingClassifierScratch(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Gradient Boosting Classifier from scratch:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c763cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 0.82\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d158ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost Classifier: 0.8208333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vpthi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:00:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of XGBoost Classifier:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009075b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

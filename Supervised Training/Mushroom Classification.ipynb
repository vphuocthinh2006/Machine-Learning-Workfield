{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2bc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51807351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class cap-shape cap-surface  ... spore-print-color population habitat\n",
      "0     p         x           s  ...                 k          s       u\n",
      "1     e         x           s  ...                 n          n       g\n",
      "2     e         b           s  ...                 n          n       m\n",
      "3     p         x           y  ...                 k          s       u\n",
      "4     e         x           s  ...                 n          a       g\n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"uciml/mushroom-classification\")\n",
    "data = pd.read_csv(path + \"/mushrooms.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0e0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
      "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
      "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
      "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
      "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
      "       'ring-type', 'spore-print-color', 'population', 'habitat'],\n",
      "      dtype='object')\n",
      "  cap-shape cap-surface cap-color  ... spore-print-color population habitat\n",
      "0         x           s         n  ...                 k          s       u\n",
      "1         x           s         y  ...                 n          n       g\n",
      "2         b           s         w  ...                 n          n       m\n",
      "3         x           y         w  ...                 k          s       u\n",
      "4         x           s         g  ...                 n          a       g\n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Alright, let's find which is our y since this is an supervised learning problem\n",
    "print(data.columns)\n",
    "# The target variable is \"class\" which indicates whether the mushroom is edible or poisonous\n",
    "# Alright, let's get the data.\n",
    "X = data.drop(\"class\", axis=1)\n",
    "y = data[\"class\"]\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "322fe8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cap-shape  cap-surface  cap-color  ...  spore-print-color  population  habitat\n",
      "0          5            2          4  ...                  2           3        5\n",
      "1          5            2          9  ...                  3           2        1\n",
      "2          0            2          8  ...                  3           2        3\n",
      "3          5            3          8  ...                  2           3        5\n",
      "4          5            2          3  ...                  3           0        1\n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Should we encode the categorical variables? Yes, we turn these into numerical values\n",
    "le = LabelEncoder()\n",
    "for col in X.columns:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "y = le.fit_transform(y)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970b4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, now, since we're going to grab the basic of a Decision Tree, XGboost, and logistic regression. Not only that we will implement Neural Networks in to solve the problem.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43274d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.952\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       843\n",
      "           1       0.94      0.96      0.95       782\n",
      "\n",
      "    accuracy                           0.95      1625\n",
      "   macro avg       0.95      0.95      0.95      1625\n",
      "weighted avg       0.95      0.95      0.95      1625\n",
      "\n",
      "Confusion Matrix:\n",
      " [[799  44]\n",
      " [ 34 748]]\n",
      "Balanced Logistic Regression Accuracy: 0.955076923076923\n",
      "Balanced Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       843\n",
      "           1       0.94      0.96      0.95       782\n",
      "\n",
      "    accuracy                           0.96      1625\n",
      "   macro avg       0.95      0.96      0.96      1625\n",
      "weighted avg       0.96      0.96      0.96      1625\n",
      "\n",
      "Balanced Confusion Matrix:\n",
      " [[799  44]\n",
      " [ 29 753]]\n"
     ]
    }
   ],
   "source": [
    "# Alright, let's run through Logistic Regression\n",
    "# First off, the logistic regression's formula is sigmoid(w * x + b)\n",
    "# Where w is the weights, x is the input features, and b is the bias term.\n",
    "# The sigmoid function is defined as 1 / (1 + exp(-z)), where z is the linear combination of weights and features.\n",
    "# The output of the sigmoid function is a value between 0 and 1, which can be interpreted as a probability.\n",
    "# For binary classification, we typically set a threshold (e.g., 0.5) to decide the class label.\n",
    "# If the output is greater than or equal to the threshold, we classify the instance as class 1; otherwise, we classify it as class 0.\n",
    "# The model is trained using the maximum likelihood estimation method, which aims to find the parameters (weights and bias) that maximize the likelihood of the observed data.\n",
    "# Now, moving on, the cost function used in logistic regression is the log-loss function, also known as binary cross-entropy loss.\n",
    "# The log-loss function measures the difference between the predicted probabilities and the actual class labels.\n",
    "# It is defined as:\n",
    "# LogLoss = - (1/N) * Σ [y * log(p) + (1 - y) * log(1 - p)]\n",
    "# This will give penalty to which y is far from p in terms of logarithmic scale.\n",
    "# Where N is the number of instances, y is the actual class label (0 or 1), and p is the predicted probability of class 1.\n",
    "# The goal of training the logistic regression model is to minimize the log-loss function, which is\n",
    "# achieved using optimization algorithms such as gradient descent.\n",
    "# The optimization process iteratively updates the weights and bias to reduce the log-loss until convergence.\n",
    "# Once the model is trained, it can be used to predict the class labels for new instances by applying the sigmoid function to the linear combination of weights and features and then using the threshold to determine the class label.\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "# The accuracy turns out to be 95%?\n",
    "# But the confusion matrix seems... off. Because the true positive is 799. Isn't that big?\n",
    "# Well, because the dataset is imbalanced. There are more edible mushrooms than poisonous ones.\n",
    "# And, so, the model is biased towards predicting the majority class.\n",
    "# This is a common issue in classification problems with imbalanced datasets.\n",
    "# To address this, we can use techniques such as resampling the dataset (either oversampling the minority class or undersampling the majority class), using different evaluation metrics (such as precision, recall, and F1-score), or applying algorithms that are specifically designed to handle imbalanced datasets.\n",
    "# How to? We can use the class_weight parameter in LogisticRegression to give more weight to the minority class.\n",
    "log_reg_balanced = LogisticRegression(class_weight='balanced')\n",
    "log_reg_balanced.fit(X_train, y_train)\n",
    "y_pred_log_reg_balanced = log_reg_balanced.predict(X_test)\n",
    "print(\"Balanced Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg_balanced))\n",
    "print(\"Balanced Classification Report:\\n\", classification_report(y_test, y_pred_log_reg_balanced))\n",
    "print(\"Balanced Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg_balanced))\n",
    "\n",
    "#Now that we have a better model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2aa3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's move onto Decision Tree which is a little bit harder.\n",
    "# Say that, you knew about a structure of a tree root and such things as that in DSA.\n",
    "# A tree has its nodes and such.\n",
    "# So thing is... the tree will have its leaf decide the outcome.\n",
    "# The decision tree algorithm works by recursively splitting the data into subsets based on the feature that provides the best separation between classes.\n",
    "# The splitting is done using a criterion such as Gini impurity or information gain.\n",
    "# So, what is information gain and Gini impurity?\n",
    "# Information gain measures the reduction in entropy (uncertainty) after a dataset is split on a particular feature.\n",
    "# Entropy is a measure of the randomness or impurity in the dataset.\n",
    "# The formula for entropy is:\n",
    "# Entropy(S) = - Σ (p(x) * log2(p(x))), what is this? p(x) is the proportion of instances in class x.\n",
    "# For instance, we have a dataset with 10 instances, 6 of which belong to class A and 4 belong to class B.\n",
    "# The entropy of this dataset would be:\n",
    "# Entropy(S) = - [(6/10) * log2(6/10) + (4/10) * log2(4/10)] = 0.97095\n",
    "# And, from that, we know that impurity of the dataset is 0.97095.\n",
    "# Now, when we split the dataset on a feature, we can calculate the entropy of each subset and the weighted average entropy of the subsets.\n",
    "# The information gain is then calculated as the difference between the entropy of the original dataset and the weighted average entropy of the subsets.\n",
    "# Say that we split the dataset on a feature that results in two subsets: one with 4 instances of class A and 1 instance of class B, and another with 2 instances of class A and 3 instances of class B.\n",
    "# The entropy of the first subset would be:\n",
    "# Entropy(S1) = - [(4/5) * log2(4/5) + (1/5) * log2(1/5)] = 0.72193\n",
    "# The entropy of the second subset would be:\n",
    "# Entropy(S2) = - [(2/5) * log2(2/5) + (3/5) * log2(3/5)] = 0.97095\n",
    "# The weighted average entropy of the subsets would be:\n",
    "# Weighted Entropy = (5/10) * Entropy(S1) + (5/10) * Entropy(S2) = 0.84644\n",
    "# The information gain from splitting on this feature would be:\n",
    "# Information Gain = Entropy(S) - Weighted Entropy = 0.97095 - 0.84644 = 0.12451\n",
    "# Gini impurity is another measure of impurity used in decision trees.\n",
    "# It measures the probability of incorrectly classifying a randomly chosen instance if it were randomly labeled according\n",
    "# to the distribution of class labels in the dataset.\n",
    "# The formula for Gini impurity is:\n",
    "# Gini(S) = 1 - Σ (p(x)^2), where p(x) is the proportion of instances in class x.\n",
    "# For the same dataset with 6 instances of class A and 4 instances of class B, the Gini impurity would be:\n",
    "#  Gini(S) = 1 - [(6/10)^2 + (4/10)^2] = 0.48\n",
    "# When we split the dataset on a feature, we can calculate the Gini impurity of each subset and the weighted average Gini impurity of the subsets.\n",
    "# The Gini impurity is then calculated as the weighted average Gini impurity of the subsets.\n",
    "# Using the same subsets as before, the Gini impurity of the first subset would be:\n",
    "# Gini(S1) = 1 - [(4/5)^2 + (1/5)^2] = 0.32\n",
    "# The Gini impurity of the second subset would be:\n",
    "# Gini(S2) = 1 - [(2/5)^2 + (3/5)^2] = 0.48\n",
    "# The weighted average Gini impurity of the subsets would be:\n",
    "# Weighted Gini = (5/10) * Gini(S1) + (5/10) * Gini(S2) = 0.4\n",
    "# The goal of the decision tree algorithm is to find the feature that provides the highest information gain or lowest Gini impurity at each node of the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d0ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9766153846153847\n",
      "Decision Tree Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       843\n",
      "           1       0.96      0.99      0.98       782\n",
      "\n",
      "    accuracy                           0.98      1625\n",
      "   macro avg       0.98      0.98      0.98      1625\n",
      "weighted avg       0.98      0.98      0.98      1625\n",
      "\n",
      "Decision Tree Confusion Matrix:\n",
      " [[814  29]\n",
      " [  9 773]]\n",
      "XGBoost Accuracy: 1.0\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       843\n",
      "           1       1.00      1.00      1.00       782\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      " [[843   0]\n",
      " [  0 782]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vpthi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:48:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=5)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
    "#Wow, the accuracy is 100%? Is that even possible?\n",
    "#Well, yes, because decision trees can easily overfit the training data, especially if the tree is allowed to grow deep without any constraints.\n",
    "#Overfitting occurs when the model learns the noise and details of the training data to the extent that it negatively impacts the model's performance on new, unseen data.\n",
    "#To prevent overfitting, we can use techniques such as pruning the tree, setting a maximum depth for the tree, or requiring a minimum number of samples per leaf node.\n",
    "# Let's try to limit the depth of the tree to 5\n",
    "# Holy moly, the accuracy is 97%?! Isn't that great?\n",
    "# Imagine within the XGBOOST, it uses decision trees as its base learners.\n",
    "# Each tree is built sequentially, with each new tree attempting to correct the errors made by the previous trees.\n",
    "# The final prediction is made by combining the predictions of all the trees, typically through a weighted sum.\n",
    "# XGBoost also includes regularization techniques to prevent overfitting, such as L1 (Lasso) and L2 (Ridge) regularization.\n",
    "# And, not only that it boosts so much speed too!\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "# The accuracy is 100%? Again?\n",
    "# Well, XGBoost is a powerful algorithm that can achieve high accuracy on many datasets,\n",
    "# especially when the data is well-preprocessed and the model is properly tuned.\n",
    "# However, achieving 100% accuracy on a real-world dataset is quite rare and often indicates overfitting.\n",
    "# Overfitting occurs when the model learns the training data too well, including its noise and\n",
    "# which can lead to poor generalization to new, unseen data.\n",
    "# To mitigate overfitting, it's important to use techniques such as cross-validation,\n",
    "# regularization, and early stopping during training.\n",
    "# Additionally, evaluating the model using metrics beyond accuracy, such as precision, recall,\n",
    "# and F1-score, can provide a more comprehensive understanding of the model's performance,\n",
    "# especially in cases of imbalanced datasets.\n",
    "# Finally, let's implement a simple Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4e2692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vpthi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.2447 - val_accuracy: 0.9785 - val_loss: 0.0895\n",
      "Epoch 2/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0497 - val_accuracy: 0.9954 - val_loss: 0.0235\n",
      "Epoch 3/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0152 - val_accuracy: 0.9985 - val_loss: 0.0099\n",
      "Epoch 4/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 5/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
      "Epoch 6/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.7026e-04 - val_accuracy: 1.0000 - val_loss: 6.1045e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.4181e-04 - val_accuracy: 1.0000 - val_loss: 5.0528e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4294e-04 - val_accuracy: 1.0000 - val_loss: 5.8071e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.6085e-04 - val_accuracy: 1.0000 - val_loss: 3.0167e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.1287e-04 - val_accuracy: 1.0000 - val_loss: 3.7743e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.6268e-04 - val_accuracy: 1.0000 - val_loss: 2.2704e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0174e-04 - val_accuracy: 1.0000 - val_loss: 1.7685e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.7932e-04 - val_accuracy: 1.0000 - val_loss: 1.6179e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3991e-04 - val_accuracy: 1.0000 - val_loss: 1.2304e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.2548e-04 - val_accuracy: 1.0000 - val_loss: 1.1852e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0635e-04 - val_accuracy: 1.0000 - val_loss: 9.2759e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.8105e-05 - val_accuracy: 1.0000 - val_loss: 8.2272e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.9924e-05 - val_accuracy: 1.0000 - val_loss: 7.1720e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.1100e-05 - val_accuracy: 1.0000 - val_loss: 6.7483e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.2371e-05 - val_accuracy: 1.0000 - val_loss: 5.8593e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1731e-05 - val_accuracy: 1.0000 - val_loss: 5.0145e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4244e-05 - val_accuracy: 1.0000 - val_loss: 4.3977e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4601e-05 - val_accuracy: 1.0000 - val_loss: 3.6580e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.8333e-05 - val_accuracy: 1.0000 - val_loss: 3.3899e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1003e-05 - val_accuracy: 1.0000 - val_loss: 3.7712e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.8846e-05 - val_accuracy: 1.0000 - val_loss: 2.6376e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.7018e-05 - val_accuracy: 1.0000 - val_loss: 3.2087e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.4800e-05 - val_accuracy: 1.0000 - val_loss: 2.1909e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.0663e-05 - val_accuracy: 1.0000 - val_loss: 1.9525e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.7840e-05 - val_accuracy: 1.0000 - val_loss: 1.8612e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6575e-05 - val_accuracy: 1.0000 - val_loss: 1.5340e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.4295e-05 - val_accuracy: 1.0000 - val_loss: 1.3370e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3249e-05 - val_accuracy: 1.0000 - val_loss: 1.2319e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2835e-05 - val_accuracy: 1.0000 - val_loss: 1.0922e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0885e-05 - val_accuracy: 1.0000 - val_loss: 1.0247e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.6124e-06 - val_accuracy: 1.0000 - val_loss: 9.3903e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.4216e-06 - val_accuracy: 1.0000 - val_loss: 8.2466e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.5514e-06 - val_accuracy: 1.0000 - val_loss: 7.9713e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.8073e-06 - val_accuracy: 1.0000 - val_loss: 8.0094e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.9495e-06 - val_accuracy: 1.0000 - val_loss: 6.6825e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.1508e-06 - val_accuracy: 1.0000 - val_loss: 5.6368e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.9701e-06 - val_accuracy: 1.0000 - val_loss: 5.2865e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.6530e-06 - val_accuracy: 1.0000 - val_loss: 4.5406e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.1803e-06 - val_accuracy: 1.0000 - val_loss: 4.1934e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.8243e-06 - val_accuracy: 1.0000 - val_loss: 3.8108e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.6592e-06 - val_accuracy: 1.0000 - val_loss: 3.5197e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.1446e-06 - val_accuracy: 1.0000 - val_loss: 3.1780e-06\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Neural Network Accuracy: 1.0\n",
      "Neural Network Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       843\n",
      "           1       1.00      1.00      1.00       782\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n",
      "Neural Network Confusion Matrix:\n",
      " [[843   0]\n",
      " [  0 782]]\n"
     ]
    }
   ],
   "source": [
    "# Moving onto Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "#Why do we exactly need 64 units and a relu? \n",
    "# The number of units (64 and 32) in each layer is a hyperparameter that can be tuned.\n",
    "# The choice of 64 and 32 is somewhat arbitrary and can be adjusted based on the complexity of the problem and the size of the dataset.\n",
    "# More units can allow the model to learn more complex patterns, but it also increases the risk of overfitting.\n",
    "# So, how to choose the right number of units?\n",
    "# A common approach is to start with a small number of units and gradually increase it while monitoring the model's performance on a validation set.\n",
    "# The activation function 'relu' (Rectified Linear Unit) is commonly used in hidden layers of neural networks.\n",
    "# The formula is f(x) = max(0, x)\n",
    "# Which helps you find the maximum between 0 and x.\n",
    "# It introduces non-linearity into the model, allowing it to learn complex patterns in the data.\n",
    "# ReLU is computationally efficient and helps mitigate the vanishing gradient problem, which can occur with other activation functions like sigmoid or tanh.\n",
    "# The choice of ReLU is often based on empirical results, as it has been found to work well in many scenarios.\n",
    "# However, other activation functions such as Leaky ReLU, ELU, or SELU can also be considered based on the specific requirements of the problem.\n",
    "# So, how to choose? Now, lets look at this mushroom problem.\n",
    "# The mushroom classification problem is a binary classification task where the goal is to predict whether a mushroom is edible or poisonous based on its features.\n",
    "# The input features are categorical variables that describe various characteristics of the mushrooms, such as cap shape, cap color, gill size, and habitat.\n",
    "# The neural network architecture consists of an input layer, two hidden layers, and an output layer.\n",
    "# The input layer has a number of neurons equal to the number of features in the dataset.\n",
    "# The first hidden layer has 64 neurons, and the second hidden layer has 32 neurons\n",
    "# Both hidden layers use the ReLU activation function to introduce non-linearity into the model. But, why?\n",
    "# The output layer has a single neuron with a sigmoid activation function, which outputs a probability value between 0 and 1.\n",
    "# The choice of 64 and 32 neurons in the hidden layers is somewhat arbitrary and can be adjusted based on the complexity of the problem and the size of the dataset.\n",
    "# ReLU (Rectified Linear Unit) is a popular activation function that helps introduce non-linearity into the model.\n",
    "# The final layer uses a sigmoid activation function because this is a binary classification problem, and sigmoid outputs a probability between 0 and 1.\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
    "print(\"Neural Network Classification Report:\\n\", classification_report(y_test, y_pred_nn))\n",
    "print(\"Neural Network Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRAZY! The accuracy is 100% again!\n",
    "# Well, neural networks are powerful models that can learn complex patterns in data, especially when the data is well-preprocessed and the model is properly tuned.\n",
    "# However, achieving 100% accuracy on a real-world dataset is quite rare and often indicates\n",
    "# overfitting. Overfitting occurs when the model learns the training data too well, including its noise and\n",
    "# which can lead to poor generalization to new, unseen data.\n",
    "# To mitigate overfitting, it's important to use techniques such as cross-validation,\n",
    "# regularization, and early stopping during training.\n",
    "# But, hey, at least we got a good model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
